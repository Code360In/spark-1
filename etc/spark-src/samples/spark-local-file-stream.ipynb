{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "972a65b0-5f52-4115-b204-e99dd0a651b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf55d19-49b4-47a0-9c61-eb7aece62bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역변수 설정 \n",
    "SPARK_APP_NAME = \"DATA-Preparation\"\n",
    "SPARK_MASTER = \"spark://34.64.108.172:7077\" #\"local[3]\"\n",
    "PORT = 9999\n",
    "HOST_NAME = socket.gethostname()\n",
    "# Define path\n",
    "DATA_ROOT = '/tf/notebooks/data'\n",
    "DATA_PATH = f'{DATA_ROOT}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6d783-8ae8-4a3e-8481-958ba29b3e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64/'\n",
    "os.environ['PATH'] = '/usr/local/sbin:/usr/local/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin/'\n",
    "os.environ['SPARK_HOME'] = '/usr/local'\n",
    "os.environ['HADOOP_HOME'] = '/hadoop-3.2.2'\n",
    "os.environ['hadoop.home.dir'] = '/hadoop-3.2.2/bin'\n",
    "os.environ['CLASSPATH'] = '$CLASSPATH:/hadoop-3.2.2/spark-3.2.0-bin-hadoop3.2.tar'\n",
    "print(os.getenv('HADOOP_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93cd502d-8e07-4c8d-8601-6ac459aa873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/sbin:/usr/local/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin/\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv('PATH'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c159efe-bcce-4be7-839e-49b0e136d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba90de2-dccc-4bf8-818d-9997819a7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.name', 'DATA-Preparation'), ('spark.app.id', 'app-20220207054443-0000'), ('spark.master', 'spark://34.64.108.172:7077'), ('spark.rdd.compress', 'True'), ('spark.driver.host', 'spark-client'), ('spark.serializer.objectStreamReset', '100'), ('spark.sql.warehouse.dir', 'file:/tf/notebooks/spark-warehouse'), ('spark.driver.port', '37247'), ('spark.submit.pyFiles', ''), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.app.startTime', '1644212683000')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(SPARK_MASTER).appName(SPARK_APP_NAME).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(sc._conf.getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1c3262-adf8-4dcf-884a-a0871b187f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session 생성 메서드 \n",
    "def init_session():\n",
    "    spark = SparkSession.builder.master(SPARK_MASTER).appName(SPARK_APP_NAME).config('spark.driver.host', HOST_NAME).getOrCreate()\n",
    "    default_conf = spark.sparkContext._conf.getAll()\n",
    "    conf = spark.sparkContext._conf.setAll([\n",
    "        ('spark.executor.instances', 1)\n",
    "        , ('spark.driver.memory', '12g'), ('spark.executor.memory', '8g'), ('spark.driver.maxResultSize', '8g')\n",
    "        , ('spark.driver.allowMultipleContexts', 'true'), ('spark.sql.shuffle.partitions', 8)\n",
    "        #,('spark.memory.offHeap.enabled', True), ('spark.memory.offHeap.size', '8g')\n",
    "    ])\n",
    "    spark.sparkContext.stop()\n",
    "    \n",
    "    spark = SparkSession.builder.master(SPARK_MASTER).appName(SPARK_APP_NAME).config(conf=default_conf).getOrCreate()\n",
    "    new_conf = spark.SparkContext._conf.getAll()\n",
    "    print(f'Updated Conf : {new_conf}')\n",
    "    return spark\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12cb1e97-b10b-4802-9412-576d43a5cb36",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spark-home\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/spark-home/./bin/spark-submit': '/spark-home/./bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c25575793a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 스파크 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo $SPARK_HOME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4b28ce7178c0>\u001b[0m in \u001b[0;36minit_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Spark session 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minit_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPARK_MASTER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPARK_APP_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.driver.host'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHOST_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdefault_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     conf = spark.sparkContext._conf.setAll([\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIG_IGN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preexec_fn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreexec_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# preexec_fn not supported on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1362\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/spark-home/./bin/spark-submit': '/spark-home/./bin/spark-submit'"
     ]
    }
   ],
   "source": [
    "# 스파크 생성 \n",
    "!echo $SPARK_HOME\n",
    "spark = init_session()\n",
    "sc = spark.sparkContext\n",
    "print(sc._conf.getAll())\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06dda262-1047-4d1c-b6e9-3dcd759ffbff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# channel, day, seg, setop, 1000\n",
    "\n",
    "# setop data n 개 만들기\n",
    "def create_setops():\n",
    "    setop_count = 10000\n",
    "    setop_name = ['ST_A', 'ST_B', 'ST_C', 'ST_D', 'ST_E']\n",
    "    setops = []\n",
    "    for s in setop_name:\n",
    "        for i in range(0, int(setop_count/len(setop_name))):\n",
    "            setops.append(f'{s}_{i:03d}')\n",
    "            \n",
    "    print(setops[-10:])\n",
    "    return setops\n",
    "\n",
    "# channel, day, seg data 생성 \n",
    "def create_others():\n",
    "    # 20\n",
    "    channels = ['KBS', 'MBC', 'SBS', 'JTBC', 'CBS' ,  'OCN', 'TVN', 'TVCH', 'BTN', 'EBS',  'Arirang', 'JTV', 'GAME-TV', 'HBC', 'BBC',  'CNN', 'CNBC', 'CCN', 'NHK', 'ABC'] \n",
    "    days = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "    hour_bands = ['00', '01', '10', '11', '23']\n",
    "    segs = ['Agriculture', 'Game']\n",
    "    rows = []\n",
    "    row = []\n",
    "    for c in channels:\n",
    "        for d in days:\n",
    "            for s in segs:\n",
    "                row = [c, d, s]\n",
    "                rows.append(row)\n",
    "                \n",
    "    print(rows[:5])\n",
    "    return rows\n",
    "\n",
    "def merge_to_inventory(setops, rows):\n",
    "    inven_time = 10000 # 하드코딩 시간\n",
    "    invens = []\n",
    "    for r in rows:\n",
    "        for s in setops:\n",
    "            invens.append(r + [s, inven_time])\n",
    "    print(f'Inventory Length : {len(invens):,}')\n",
    "    return invens\n",
    "\n",
    "def define_schema():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "    columns = [\n",
    "        StructField(\"channel\", StringType())\n",
    "        , StructField(\"day\", StringType())\n",
    "        , StructField(\"seg\", StringType())\n",
    "        , StructField(\"setop\", StringType())\n",
    "        , StructField(\"remains\", LongType())\n",
    "    ]\n",
    "    inven_schema = StructType(columns)\n",
    "    return inven_schema\n",
    "\n",
    "def save_inventory(invens, spark_session=spark, file_name=f'{DATA_PATH}/inven', sample_count=10000):\n",
    "    inven_schema = define_schema()\n",
    "    if (sample_count <= 0):\n",
    "        # all data \n",
    "        rdd = spark_session.sparkContext.parallelize(invens)\n",
    "    else:\n",
    "        # sampling data\n",
    "        rdd = spark_session.sparkContext.parallelize(invens[:sample_count])\n",
    "    df = spark_session.createDataFrame(rdd, inven_schema)\n",
    "    df.write.save(path=file_name, format='csv', mode='append', sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01fe5efd-82a3-4893-a6f1-dfe3c5881719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ST_E_1990', 'ST_E_1991', 'ST_E_1992', 'ST_E_1993', 'ST_E_1994', 'ST_E_1995', 'ST_E_1996', 'ST_E_1997', 'ST_E_1998', 'ST_E_1999']\n",
      "CPU times: user 8.11 ms, sys: 0 ns, total: 8.11 ms\n",
      "Wall time: 8.41 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "setops = create_setops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cd00cb2-3971-4d50-84bc-43ca2ad91b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['KBS', 'mon', 'Agriculture'], ['KBS', 'mon', 'Game'], ['KBS', 'tue', 'Agriculture'], ['KBS', 'tue', 'Game'], ['KBS', 'wed', 'Agriculture']]\n",
      "CPU times: user 1.63 ms, sys: 0 ns, total: 1.63 ms\n",
      "Wall time: 1.74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rows = create_others()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f429cf9-c715-438a-8102-6b21ec669e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory Length : 2,800,000\n",
      "CPU times: user 1.97 s, sys: 257 ms, total: 2.22 s\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "invens = merge_to_inventory(setops, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42b4b5-5ad8-4f63-949a-abe462888ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLING_COUNT : 10,000,000\n",
      "TABLE_NAME : /tf/notebooks/data/inven\n",
      "Inventory Length : 2,800,000\n"
     ]
    }
   ],
   "source": [
    "# invens data 파일로 저장하기 \n",
    "SAMPLING_COUNT = int(1e7)\n",
    "TABLE_NAME = f'{DATA_PATH}/inven'\n",
    "\n",
    "print(f'SAMPLING_COUNT : {SAMPLING_COUNT:,}')\n",
    "print(f'TABLE_NAME : {TABLE_NAME}')\n",
    "print(f'Inventory Length : {len(invens):,}')\n",
    "      \n",
    "save_inventory(invens, spark, TABLE_NAME, SAMPLING_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec032c3-c1b4-4b9c-a9a5-10573125f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 결과 확인하기 \n",
    "lines = spark.read.format('csv').schema(define_schema()).option('path', TABLE_NAME).load()\n",
    "data_count = lines.count()\n",
    "print(f'DATA Count : {data_count:,}')\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289ac29-b2ab-4d73-9416-f65cdcc8e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch 별로 setop 번호 순서 5개씩 보여주기\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number \n",
    "\n",
    "windowPart = Window.partitionBy('channel').orderBy(col('setop').desc())\n",
    "lines = spark.read.format('csv').schema(define_schema()).option('path', TABLE_NAME).load()\n",
    "df2 = lines.withColumn('row', row_number().over(windowPart)).filter('row') <= 5)\n",
    "df2.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12be67-7635-4146-ac92-41787237a108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
