{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d707f6-8615-4284-90c9-af0973b2811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5MB)\n",
      "\u001b[K    100% |################################| 9.5MB 184kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d49e7-f3c5-4588-981b-fae4388092c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38e2a6d2-a575-4c3e-90fa-bc532a0a1829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.pandas.utils import require_minimum_pandas_version, require_minimum_pyarrow_version\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Arrow-in-Spark example\") \\\n",
    "        .getOrCreate()\n",
    "# builder.master(\"local[1]\") \\ \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b32e7153-da5f-4c98-bbd6-7d032589321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51f16f9-7526-46f8-8ce8-c38efa13e202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-client:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Arrow-in-Spark example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb0ee036ac8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b3cd9f-b7ed-4aaa-b737-2112b3ec82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/sql/pandas/conversion.py:329: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/pyspark/sql/pandas/conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame result statistics:\n",
      "                0           1           2\n",
      "count  100.000000  100.000000  100.000000\n",
      "mean     0.493640    0.507350    0.530803\n",
      "std      0.276708    0.293061    0.279217\n",
      "min      0.028506    0.022717    0.002653\n",
      "25%      0.268534    0.270429    0.286999\n",
      "50%      0.497026    0.499115    0.545593\n",
      "75%      0.704514    0.791336    0.755214\n",
      "max      0.991715    0.984116    0.979381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # type: ignore[import]\n",
    "import pandas as pd  # type: ignore[import]\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "\n",
    "print(\"Pandas DataFrame result statistics:\\n%s\\n\" % str(result_pdf.describe()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c01857-83fb-4693-8b8f-1eca567158f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.493640</td>\n",
       "      <td>0.507350</td>\n",
       "      <td>0.530803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.276708</td>\n",
       "      <td>0.293061</td>\n",
       "      <td>0.279217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.028506</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.268534</td>\n",
       "      <td>0.270429</td>\n",
       "      <td>0.286999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.497026</td>\n",
       "      <td>0.499115</td>\n",
       "      <td>0.545593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.704514</td>\n",
       "      <td>0.791336</td>\n",
       "      <td>0.755214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.991715</td>\n",
       "      <td>0.984116</td>\n",
       "      <td>0.979381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     0.493640    0.507350    0.530803\n",
       "std      0.276708    0.293061    0.279217\n",
       "min      0.028506    0.022717    0.002653\n",
       "25%      0.268534    0.270429    0.286999\n",
       "50%      0.497026    0.499115    0.545593\n",
       "75%      0.704514    0.791336    0.755214\n",
       "max      0.991715    0.984116    0.979381"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c39647-0be2-4c39-a0ec-e4d0e5ee13c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"data/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2db8c20-6247-4669-859d-3cfec8d682c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd20b155-1d93-4084-a28c-2c00ba07c486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([\"age\", \"name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e4c37c5-4282-48b2-a433-89ebeac70fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|(age + 1)| age|\n",
      "+---------+----+\n",
      "|     null|null|\n",
      "|       31|  30|\n",
      "|       20|  19|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([df[\"age\"]+1, \"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f67ba1c1-7bdf-48c7-a7c9-6cd4f42d0a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12b9a99e-18b4-4f04-9ce2-3bb6f28c27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"people2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea35c18b-249f-4340-a045-f95f69f9e9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT age FROM people2\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "826144fd-e612-4717-b37e-ad68ff5a4ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data/people.txt MapPartitionsRDD[58] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(\"data/people.txt\")\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "915b84df-7457-43c7-90af-6a302aa6c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "lines.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62dbe906-9cf3-4e3e-ab79-19ebe04b138a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Andy, 30', 'Justin, 19']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f66fbaff-70ff-48d7-b510-0f7cf43927bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael, 29\n",
      "Andy, 30\n",
      "Justin, 19\n"
     ]
    }
   ],
   "source": [
    "for x in lines.collect():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c28cd3fb-e932-4eae-bd5f-c8a897635a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Michael, 29', 'Andy, 30']\n"
     ]
    }
   ],
   "source": [
    "print(lines.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bab3e20c-dfb9-4fe9-bfa4-2a9627025cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael, 29', 'Andy, 30', 'Justin, 19']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41d2b44b-e799-4b72-a3ea-394cc4cb305e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('29', 1),\n",
       " ('Andy,', 1),\n",
       " ('Justin,', 1),\n",
       " ('19', 1),\n",
       " ('Michael,', 1),\n",
       " ('30', 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = lines.flatMap(lambda x: x.split(' '))\n",
    "map = data.map(lambda x: (x, 1))\n",
    "mapreduce = map.reduceByKey(lambda x,y: x+y)\n",
    "result = mapreduce.collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "508f9ad4-9157-4187-92df-b079afc5df52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael,', '29', 'Andy,', '30', 'Justin,', '19']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = lines.flatMap(lambda x: x.split(' '))\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1b24a41-a425-4029-aadc-d9cf7921f7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[81] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4093072-60a3-4503-b2ab-e058a077c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsSequenceFile(\"data/seq_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90b424af-5bf6-4278-be8e-95f9e29d7586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(spark.sparkContext.sequenceFile(\"data/seq_file\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7190ff91-942e-4b71-9b92-b7bf2a490c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4f52c09d-392a-46b4-8230-3da18cb60ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"data/people.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "totalLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92088bc7-d946-4679-afea-87fe3b2a756c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 8, 10]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "50f68553-84b5-497c-9192-e16a62b5a746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[106] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c810350e-7d6c-4807-ac50-b2fdc3f1c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineLengths5 = lineLengths.map(lambda x: x+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "328013b4-f5be-430a-b72a-9610fee1a1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 13, 15]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths5.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f931e4e-c676-4e6a-93e0-7c2439957674",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "618b0357-4143-411d-a4cb-2a8aef1ff311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value:  0\n"
     ]
    }
   ],
   "source": [
    "data = np.arange(1, 100)\n",
    "\n",
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "    print(counter)\n",
    "\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865cb1dd-a71f-49eb-8163-d2e73a4ec337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 12), ('b', 6), ('c', 3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working with Key-Value Pairs\n",
    "#lines = sc.textFile(\"data.txt\")\n",
    "lines = sc.parallelize(['a', 'b', 'a', 'a', 'a', 'b', 'c', 'a', 'b', 'a', 'a', 'a', 'b', 'c', 'a', 'b', 'a', 'a', 'a', 'b', 'c'])\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73f34b34-65ee-44ab-8650-203cbba70bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnts = pairs.groupByKey()\n",
    "# cnts = cnts.collect()\n",
    "lst = cnts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f73179b-5631-4911-a3e2-eb9d200f19cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " ('b', [1, 1, 1, 1, 1, 1]),\n",
       " ('c', [1, 1, 1])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.groupByKey().map(lambda x : (x[0], list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49d8bb47-69d2-44c4-bb7c-7559de26200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in lst[1][1] : \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c57fe0a6-a1bb-46ba-89f7-ea20b5b66619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " ('b', [1, 1, 1, 1, 1, 1]),\n",
       " ('c', [1, 1, 1])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b96bde7c-770b-4138-a2ba-7a5bebfa4d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', <pyspark.resultiterable.ResultIterable at 0x7f9e39666240>],\n",
       " ['b', <pyspark.resultiterable.ResultIterable at 0x7f9e396cf240>],\n",
       " ['c', <pyspark.resultiterable.ResultIterable at 0x7f9e396cf048>]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.groupByKey().map(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c419ed8f-bc11-4583-a11e-b06fd8c47310",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaPeople = spark.createDataFrame(pairs)\n",
    "schemaPeople.createOrReplaceTempView(\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48a8c3a7-cbac-44dd-971d-4ec991bdf57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teenagers = spark.sql(\"SELECT * FROM pairs\")\n",
    "teenagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c2f0dcd-362f-4ca3-a174-94804dcbaace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='a', _2=1),\n",
       " Row(_1='b', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='b', _2=1),\n",
       " Row(_1='c', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='b', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='b', _2=1),\n",
       " Row(_1='c', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='b', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='a', _2=1),\n",
       " Row(_1='b', _2=1),\n",
       " Row(_1='c', _2=1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teenagers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a75b5cf6-fe33-45ce-839e-61c3f2c6e43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name: a',\n",
       " 'Name: b',\n",
       " 'Name: a',\n",
       " 'Name: a',\n",
       " 'Name: a',\n",
       " 'Name: b',\n",
       " 'Name: c',\n",
       " 'Name: a',\n",
       " 'Name: b',\n",
       " 'Name: a',\n",
       " 'Name: a',\n",
       " 'Name: a',\n",
       " 'Name: b',\n",
       " 'Name: c',\n",
       " 'Name: a',\n",
       " 'Name: b',\n",
       " 'Name: a',\n",
       " 'Name: a',\n",
       " 'Name: a',\n",
       " 'Name: b',\n",
       " 'Name: c']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p._1).collect()\n",
    "teenNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9840ecd-f127-409f-a842-7e3727546ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: a\n",
      "Name: b\n",
      "Name: a\n",
      "Name: a\n",
      "Name: a\n",
      "Name: b\n",
      "Name: c\n",
      "Name: a\n",
      "Name: b\n",
      "Name: a\n",
      "Name: a\n",
      "Name: a\n",
      "Name: b\n",
      "Name: c\n",
      "Name: a\n",
      "Name: b\n",
      "Name: a\n",
      "Name: a\n",
      "Name: a\n",
      "Name: b\n",
      "Name: c\n"
     ]
    }
   ],
   "source": [
    "for name in teenNames:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05394940-d67a-46b3-b05c-c1de1a5cc641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, count: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "schemaString = \"name count\"\n",
    "fields = [StructField(fname, StringType(), True) for fname in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "schemaPeople = spark.createDataFrame(pairs, schema)\n",
    "schemaPeople"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fd60fdd-85b8-44b6-9d67-3367a5fda01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='a', count='1'),\n",
       " Row(name='b', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='b', count='1'),\n",
       " Row(name='c', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='b', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='b', count='1'),\n",
       " Row(name='c', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='b', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='a', count='1'),\n",
       " Row(name='b', count='1'),\n",
       " Row(name='c', count='1')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople.createOrReplaceTempView(\"pairs_scheme\")\n",
    "queried = spark.sql(\"select * from pairs_scheme\")\n",
    "queried.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4471b90-e7c4-4e1f-8c2e-017c177af6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[name: string, count: string]\n",
      "+----+-----+\n",
      "|name|count|\n",
      "+----+-----+\n",
      "|   a|    1|\n",
      "|   b|    1|\n",
      "|   a|    1|\n",
      "|   a|    1|\n",
      "|   a|    1|\n",
      "|   b|    1|\n",
      "|   c|    1|\n",
      "|   a|    1|\n",
      "|   b|    1|\n",
      "|   a|    1|\n",
      "|   a|    1|\n",
      "|   a|    1|\n",
      "|   b|    1|\n",
      "|   c|    1|\n",
      "|   a|    1|\n",
      "|   b|    1|\n",
      "|   a|    1|\n",
      "|   a|    1|\n",
      "|   a|    1|\n",
      "|   b|    1|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queried)\n",
    "queried.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0814a16-cb82-4cf9-8b74-e4cd13271da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, count: string]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c5b0839-b84a-4b53-a38e-567991c73947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1',\n",
       " 'b1',\n",
       " 'a1',\n",
       " 'a1',\n",
       " 'a1',\n",
       " 'b1',\n",
       " 'c1',\n",
       " 'a1',\n",
       " 'b1',\n",
       " 'a1',\n",
       " 'a1',\n",
       " 'a1',\n",
       " 'b1',\n",
       " 'c1',\n",
       " 'a1',\n",
       " 'b1',\n",
       " 'a1',\n",
       " 'a1',\n",
       " 'a1',\n",
       " 'b1',\n",
       " 'c1']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queried.rdd.map(lambda x: x.name + x['count']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "703bc96c-f32a-4c6e-a1d8-8c8d8064b479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_collect_as_arrow',\n",
       " '_jcols',\n",
       " '_jdf',\n",
       " '_jmap',\n",
       " '_jseq',\n",
       " '_lazy_rdd',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " '_schema',\n",
       " '_sort_cols',\n",
       " '_support_repr_html',\n",
       " '_to_corrected_pandas_type',\n",
       " 'agg',\n",
       " 'alias',\n",
       " 'approxQuantile',\n",
       " 'cache',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'colRegex',\n",
       " 'collect',\n",
       " 'columns',\n",
       " 'corr',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'createGlobalTempView',\n",
       " 'createOrReplaceGlobalTempView',\n",
       " 'createOrReplaceTempView',\n",
       " 'createTempView',\n",
       " 'crossJoin',\n",
       " 'crosstab',\n",
       " 'cube',\n",
       " 'describe',\n",
       " 'distinct',\n",
       " 'drop',\n",
       " 'dropDuplicates',\n",
       " 'drop_duplicates',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'exceptAll',\n",
       " 'explain',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'freqItems',\n",
       " 'groupBy',\n",
       " 'groupby',\n",
       " 'head',\n",
       " 'hint',\n",
       " 'inputFiles',\n",
       " 'intersect',\n",
       " 'intersectAll',\n",
       " 'isLocal',\n",
       " 'isStreaming',\n",
       " 'is_cached',\n",
       " 'join',\n",
       " 'limit',\n",
       " 'localCheckpoint',\n",
       " 'mapInPandas',\n",
       " 'na',\n",
       " 'orderBy',\n",
       " 'persist',\n",
       " 'printSchema',\n",
       " 'randomSplit',\n",
       " 'rdd',\n",
       " 'registerTempTable',\n",
       " 'repartition',\n",
       " 'repartitionByRange',\n",
       " 'replace',\n",
       " 'rollup',\n",
       " 'sameSemantics',\n",
       " 'sample',\n",
       " 'sampleBy',\n",
       " 'schema',\n",
       " 'select',\n",
       " 'selectExpr',\n",
       " 'semanticHash',\n",
       " 'show',\n",
       " 'sort',\n",
       " 'sortWithinPartitions',\n",
       " 'sql_ctx',\n",
       " 'stat',\n",
       " 'storageLevel',\n",
       " 'subtract',\n",
       " 'summary',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'toDF',\n",
       " 'toJSON',\n",
       " 'toLocalIterator',\n",
       " 'toPandas',\n",
       " 'to_koalas',\n",
       " 'to_pandas_on_spark',\n",
       " 'transform',\n",
       " 'union',\n",
       " 'unionAll',\n",
       " 'unionByName',\n",
       " 'unpersist',\n",
       " 'where',\n",
       " 'withColumn',\n",
       " 'withColumnRenamed',\n",
       " 'withWatermark',\n",
       " 'write',\n",
       " 'writeStream',\n",
       " 'writeTo']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(queried)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce40397-dd2a-4607-80e3-5dfed1d800a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
