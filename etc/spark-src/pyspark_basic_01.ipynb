{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/0e/4528c9703863fc4df49fd4425c6f15ee5f370cff9e43cea3f8076b034e3f/pyspark-3.2.0.tar.gz (281.3MB)\n",
      "\u001b[K    100% |################################| 281.3MB 6.2kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting py4j==0.10.9.2 (from pyspark)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/e2/543019a6e620b759a59f134158b4595766f9bf520a1081a2ba1a1809ba32/py4j-0.10.9.2-py2.py3-none-any.whl (198kB)\n",
      "\u001b[K    100% |################################| 204kB 3.8MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e2/8e/91/cf48adee149561846ba332c9c6bb8207385a76c5e8e67c197b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "# install pyspark  : for local mode and client code\n",
    "!pip install pyspark numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "# assume that jdk already installed \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "#conf = SparkConf()  # create the configuration\n",
    "# conf.set(\"spark.driver.extraClassPath\", \"/tf/notebooks/mysql-connector-java-8.0.17.jar\")  # set the spark.jars\n",
    "# conf.set(\"spark.jars\", \"/tf/notebooks/mysql-connector-java-8.0.17.jar\")  # set the spark.jars\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[3]\").appName('LocalMode_Test').getOrCreate()\n",
    "#spark = SparkSession.builder.config(conf=conf).master(\"local[3]\").appName('LocalMode_Test').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------------------------------+\n",
      "|                               _c0|                              _c1|\n",
      "+----------------------------------+---------------------------------+\n",
      "|                                id|                          summary|\n",
      "|                         250366726|당진축협 사료공장 신축 이전이 ...|\n",
      "|  이들은 또한 “가학리 이장을 비...|                             null|\n",
      "| 지난해 8월 송악읍사무소에서 토...|   서명한 것을 두고 2반 주민들...|\n",
      "| 이들은 “사료공장 이전을 반대하...| 반대의사를 표명하기 위해 걸어...|\n",
      "| 주민들은 “앞으로도 주민들의 요...|                             null|\n",
      "|                         251994086|    남송 채규선 작가가 2016 소...|\n",
      "|   채 작가는 지난달 10일 당진문...|               한시집을 출간했다.|\n",
      "|소외예술 활성화 지원사업 시나브...|                             null|\n",
      "| 남송 채규선 작가가 발간한 책은...|                             null|\n",
      "|   채 작가는 “한시는 한문의 꽃”...|                             null|\n",
      "|       이어 “어릴 적 80~90대 지...|                             null|\n",
      "|             >>남송 채규선 작가는\"|                             null|\n",
      "|                         252639804| 정 주무관은 “시민들이 모르고 ...|\n",
      "|    한편 정 주무관은 2009년부터...|                             null|\n",
      "|  어렸을 때 뉴스와 신문을 자주 ...|                             null|\n",
      "| 정 주무관은 공무원인 만큼 시정...|                             null|\n",
      "|  정 주무관은 “공무원이다 보니 ...|                             null|\n",
      "| 한편 정 주무관은 주택가격이나 ...|  경제동향 등 시민들에게 와 닿...|\n",
      "|                         252639915| 스리랑카 공항에 내려 8시간 동...|\n",
      "+----------------------------------+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.csv(\"extraction_1125.csv\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17608"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"_c0\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#./bin/spark-submit examples/src/main/python/pi.py 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "class T():\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.end = time()\n",
    "        elapsed = self.end - self.start\n",
    "        print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 4.29 µs\n",
      "Pi is roughly 3.142782\n",
      "0:00:01.528637\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "import random\n",
    "\n",
    "NUM_SAMPLES = 10000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "with T():\n",
    "    count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.chdir(\"/tf/notebooks/client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 4.29 µs\n",
      "[Row(line='지난해 8월 송악읍사무소에서 토지개발업체인 G업체와 가학리 이장 등 주민대표들이 주민동의·합의서를 작성, 서명한 것을 두고 2반 주민들은 “사료공장 이전에 따른 피해를 가장 많이 입게 될 가학리 2반 주민들의 의견이 전혀 반영되지 않은 채 일방적으로 추진한 사항”이라며 “‘사업설명회를 진행했지만 공장 입주를 반대하는 주민들이 참여하지 않은 것’이라는 G업체 측의 주장은 사실과 다르다”고 말했다.'), Row(line='지난해 8월 ‘길거리 성희롱 처벌법’이 통과됐다.'), Row(line='학생들은 지난해 8월부터 학과장에 부임한 A교수가 학생회비 납부를 종용했다고 밝혔다.\"'), Row(line='337755248,\"지난해 8월 발매한 리패키지 앨범 LOVE YOURSELF 結 ‘Answer’의 타이틀곡 ‘IDOL’ 뮤직비디오는 16일 오후 5시 4분경 유튜브 조회수 4억 건을 넘었다.'), Row(line='338419522,\"시교육청은 교육공동체가 공감하고 바라는 교육사업 중심의 읍면지역 교육력 강화를 위해 지난해 8월 \\'읍면지역 교육력 제고 추진계획\\'을 수립하고 읍·면지역 학교의 교직원 및 학부모, 세종시민으로 구성된 제1기 읍면지역 교육발전협의회를 출범했다.'), Row(line='339758912,\"지난해 8월 8일 오후 1시 50분께 A(82)씨는 인천시 연수구의 자신의 집에서 공원에서 잡아온 비둘기를 손질하는 아들을 보고 깜짝 놀랐다.'), Row(line='환경운동연합 관계자는 “지난해 8월부터 커피전문점 매장 내 일회용 플라스틱컵 사용 금지 지도·단속이 강화됨에 따라 이를 다회용 컵으로 전환하려는 노력이 증가하고 있다”며 “환경부·지자체의 규제 의지와 제도개선에 따라 일회용품 감소폭이 큰 영향을 받고, 시민 의식개선 및 행동변화를 이끌어낼 수 있음이 확인된다.\"'), Row(line='지난해 8월 남성들을 위한 클래식 스킨케어 ‘MLB GROO’ (엠엘비 그루) 라인을 성공적으로 런칭한 바 있는 MLB(엠엘비) 코스메틱은 액티브 라이프스타일과 워크아웃에 최적화된 실용적인 라인들로 구성돼 건강한 피부 컨디션과 밸런스 케어에 최적화된 액티브뷰티(Active Beauty)를 추구하는 브랜드다.'), Row(line='355670758,\"전력거래소 제주지사는 7일 오후 6시 기준 제주의 전력수요는 지난해 8월 6일의 94만9000㎾ 보다 9000㎾ 많은 95만8000㎾를 기록했다고 밝혔다.'), Row(line='이를 일평균으로 환산하면 1067만9762 홍콩달러(약 16억4265만원)로, 지난해 8월 일평균 환전액 1239만9076 홍콩달러(약 19억710만원)보다 13.9% 급감한 것이다.'), Row(line='360263983,\"18일 한국공항공사 항공통계에 따르면 올해 1∼8월 대구공항 국제선 운항편수는 1만3천376편으로 지난해 같은 기간(8천631편)보다 55％ 증가했다.')]\n",
      "0:00:00.427888\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "with T():\n",
    "    textFile = sc.textFile(\"data/extraction_1125.csv\")\n",
    "\n",
    "    # Creates a DataFrame having a single column named \"line\"\n",
    "    df = textFile.map(lambda r: Row(r)).toDF([\"line\"])\n",
    "    errors = df.filter(col(\"line\").like(\"%8월%\"))\n",
    "    # Counts all the errors\n",
    "    errors.count()\n",
    "\n",
    "    finds = errors.filter(col(\"line\").like(\"%지난해%\")).collect()\n",
    "    print(finds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, (11, 1))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "len(finds), np.shape(finds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe & Database connection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceLineId: int, InvoiceId: int, TrackId: int, UnitPrice: double, Quantity: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a DataFrame based on a table named \"people\"\n",
    "# stored in a MySQL database.\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "url = \\\n",
    "  \"jdbc:mysql://172.17.0.3:3306/test\"\n",
    "df = sqlContext \\\n",
    "  .read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", url) \\\n",
    "  .option(\"user\", \"root\") \\\n",
    "  .option(\"password\", \"root\") \\\n",
    "  .option(\"dbtable\", \"InvoiceLine\") \\\n",
    "  .load()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceLineId: integer (nullable = true)\n",
      " |-- InvoiceId: integer (nullable = true)\n",
      " |-- TrackId: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      "\n",
      "2240\n",
      "+---------+-----+\n",
      "|InvoiceId|count|\n",
      "+---------+-----+\n",
      "|      148|    2|\n",
      "|      243|   14|\n",
      "|      392|    2|\n",
      "|       31|    6|\n",
      "|       85|    2|\n",
      "|      137|    9|\n",
      "|      251|    1|\n",
      "|       65|    4|\n",
      "|       53|    9|\n",
      "|      255|    6|\n",
      "|      133|    2|\n",
      "|      296|    4|\n",
      "|       78|    2|\n",
      "|      322|    2|\n",
      "|      321|    1|\n",
      "|      362|   14|\n",
      "|      375|    9|\n",
      "|      108|    6|\n",
      "|      155|    2|\n",
      "|       34|    1|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looks the schema of this DataFrame.\n",
    "df.printSchema()\n",
    "\n",
    "print(df.count())\n",
    "\n",
    "# Counts people by age\n",
    "countsByAge = df.groupBy(\"InvoiceId\").count()\n",
    "countsByAge.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o111.save.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n\t... 24 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6db6df710ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Saves countsByAge to S3 in the JSON format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcountsByAge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://aa.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o111.save.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)\n\t... 24 more\n"
     ]
    }
   ],
   "source": [
    "# Saves countsByAge to S3 in the JSON format.\n",
    "countsByAge.write.format(\"json\").save(\"s3a://aa.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\n",
      "\u001b[K    100% |################################| 13.4MB 130kB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.19.5\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
