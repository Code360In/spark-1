{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be24a65-5c61-440e-bef8-318a13f83843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spark\n"
     ]
    }
   ],
   "source": [
    "from create_spark import init_local_session, init_remote_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7f702ce-629c-4744-b27e-72f0caf14716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from os.path import abspath\n",
    "import findspark\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9094c1-9845-4989-985b-b2cb990a97ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 스파크 생성 \n",
    "# #spark = init_local_session()\n",
    "# spark = SparkSession.builder.master('yarn').appName('SPARK_APP_NAME')\\\n",
    "#     .config(\"spark.sql.warehouse.dir\", '/user/hive/warehouse')\\\n",
    "#     .config(\"spark.security.credentials.hiveserver2.enabled\", 'false')\\\n",
    "#     .config(\"spark.sql.hive.hiveserver2.jdbc.url\", 'jdbc:hive2://dn01:10000/metastore;user=hive;password=hive')\\\n",
    "#     .enableHiveSupport().getOrCreate()\n",
    "#     #.config(\"spark.datasource.hive.warehouse.metastoreUri\", 'thrift://localhost:9083')\\\n",
    "# # spark = init_remote_session(enable_hive=True)\n",
    "# sc = spark.sparkContext\n",
    "# sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b10179f-8739-4f95-a045-2d855e012749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DATA-Preparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=DATA-Preparation>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 스파크 생성 \n",
    "# spark = SparkSession.builder.master('yarn').appName('SPARK_APP_NAME')\\\n",
    "#     .config(\"spark.sql.warehouse.dir\", '/user/hive/warehouse')\\\n",
    "#     .getOrCreate()\n",
    "#     #.config(\"spark.datasource.hive.warehouse.metastoreUri\", 'thrift://localhost:9083')\\\n",
    "spark = init_remote_session()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29da154e-c1ff-440c-a56c-48b99f559747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.text(\"hdfs://namenode:9000/user/hive/warehouse/testdb.db/intbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35a188a-a399-433e-8fdf-66c8b316dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\") # Hive support is required\n",
    "# spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714c953e-b37e-41d6-87b1-e3ee41af5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"select * from testdb.intbl;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "347161b4-2fc5-4195-8c60-de2d3a705967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.text(\"hdfs://namenode:9000/user/hive/warehouse/testdb.db/intbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e5b9864-cd0b-4ef7-9e0f-5b596c2ab66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "# DATA_ROOT = f'{os.getcwd()}/data'\n",
    "DATA_PATH = '/user/root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec19233-a811-4cdc-b730-a59d61f4746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel, day, seg, setop, 1000\n",
    "# FILE_FORMAT = 'parquet'\n",
    "FILE_FORMAT = 'csv'\n",
    "\n",
    "# setop data n 개 만들기\n",
    "def create_setops():\n",
    "    setop_count = 10000\n",
    "    setop_name = ['ST_A', 'ST_B', 'ST_C', 'ST_D', 'ST_E']\n",
    "    setops = []\n",
    "    for s in setop_name:\n",
    "        for i in range(0, int(setop_count/len(setop_name))):\n",
    "            setops.append(f'{s}_{i:03d}')\n",
    "            \n",
    "    print(setops[-10:])\n",
    "    return setops\n",
    "\n",
    "# channel, day, seg data 생성 \n",
    "def create_others():\n",
    "    # 20\n",
    "    channels = ['KBS', 'MBC', 'SBS', 'JTBC', 'CBS' ,  'OCN', 'TVN', 'TVCH', 'BTN', 'EBS',  'Arirang', 'JTV', 'GAME-TV', 'HBC', 'BBC',  'CNN', 'CNBC', 'CCN', 'NHK', 'ABC'] \n",
    "    days = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "    hour_bands = ['00', '01', '10', '11', '23']\n",
    "    segs = ['Agriculture', 'Game']\n",
    "    rows = []\n",
    "    row = []\n",
    "    for c in channels:\n",
    "        for d in days:\n",
    "            for s in segs:\n",
    "                row = [c, d, s]\n",
    "                rows.append(row)\n",
    "                \n",
    "    print(rows[:5])\n",
    "    return rows\n",
    "\n",
    "def merge_to_inventory(setops, rows):\n",
    "    inven_time = 10000 # 하드코딩 시간\n",
    "    invens = []\n",
    "    for r in rows:\n",
    "        for s in setops:\n",
    "            invens.append(r + [s, inven_time])\n",
    "    print(f'Inventory Length : {len(invens):,}')\n",
    "    return invens\n",
    "\n",
    "def define_schema():\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "    columns = [\n",
    "        StructField(\"channel\", StringType())\n",
    "        , StructField(\"day\", StringType())\n",
    "        , StructField(\"seg\", StringType())\n",
    "        , StructField(\"setop\", StringType())\n",
    "        , StructField(\"remains\", LongType())\n",
    "    ]\n",
    "    inven_schema = StructType(columns)\n",
    "    return inven_schema\n",
    "\n",
    "def save_inventory(invens, spark_session=spark, file_name=f'{DATA_PATH}/inven', sample_count=10000):\n",
    "    inven_schema = define_schema()\n",
    "    if (sample_count <= 0):\n",
    "        # all data \n",
    "        rdd = spark_session.sparkContext.parallelize(invens)\n",
    "    else:\n",
    "        # sampling data\n",
    "        rdd = spark_session.sparkContext.parallelize(invens[:sample_count])\n",
    "    df = spark_session.createDataFrame(rdd, inven_schema)\n",
    "    #df.write.save(path=file_name, format='csv', mode='append', sep=',')\n",
    "    df.write.save(path=file_name, format=FILE_FORMAT, mode='append', sep=',')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c695aae-72c5-4cbf-b022-3168a04691b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ST_E_1990', 'ST_E_1991', 'ST_E_1992', 'ST_E_1993', 'ST_E_1994', 'ST_E_1995', 'ST_E_1996', 'ST_E_1997', 'ST_E_1998', 'ST_E_1999']\n",
      "[['KBS', 'mon', 'Agriculture'], ['KBS', 'mon', 'Game'], ['KBS', 'tue', 'Agriculture'], ['KBS', 'tue', 'Game'], ['KBS', 'wed', 'Agriculture']]\n"
     ]
    }
   ],
   "source": [
    "setops = create_setops()\n",
    "rows = create_others()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3240cc35-ec7d-49be-a4b5-df30e483ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory Length : 2,800,000\n",
      "CPU times: user 1.74 s, sys: 229 ms, total: 1.97 s\n",
      "Wall time: 1.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "invens = merge_to_inventory(setops, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078a7e90-12e2-473c-959e-5bab368dd873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLING_COUNT : 10,000,000\n",
      "TABLE_NAME : hdfs://namenode:9000/user/root/inven/inv\n",
      "Inventory Length : 2,800,000\n",
      "CPU times: user 1.09 s, sys: 91.2 ms, total: 1.18 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# invens data 파일로 저장하기 \n",
    "# gcp vm local : 12 s. csv - 90 MB, parq - 4.5 MB, 280만 rows \n",
    "SAMPLING_COUNT = int(1e7)\n",
    "TABLE_NAME = \"hdfs://namenode:9000/user/root/inven/inv\"\n",
    "# need file permission  \n",
    "#TABLE_NAME = f'file:///home'\n",
    "\n",
    "print(f'SAMPLING_COUNT : {SAMPLING_COUNT:,}')\n",
    "print(f'TABLE_NAME : {TABLE_NAME}')\n",
    "print(f'Inventory Length : {len(invens):,}')\n",
    "      \n",
    "save_inventory(invens, spark, TABLE_NAME, SAMPLING_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b372d4-921b-494b-a754-4909dd63ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA Count : 2,800,010\n",
      "+-------+---+----+---------+-------+\n",
      "|channel|day| seg|    setop|remains|\n",
      "+-------+---+----+---------+-------+\n",
      "|    EBS|sun|Game|ST_E_1808|  10000|\n",
      "|    EBS|sun|Game|ST_E_1809|  10000|\n",
      "|    EBS|sun|Game|ST_E_1810|  10000|\n",
      "|    EBS|sun|Game|ST_E_1811|  10000|\n",
      "|    EBS|sun|Game|ST_E_1812|  10000|\n",
      "+-------+---+----+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 3.28 ms, sys: 3.64 ms, total: 6.92 ms\n",
      "Wall time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# 저장 결과 확인하기 \n",
    "lines = spark.read.format(FILE_FORMAT).schema(define_schema()).option('path', TABLE_NAME).load()\n",
    "data_count = lines.count()\n",
    "print(f'DATA Count : {data_count:,}')\n",
    "lines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "200bcd67-22be-4d7e-b943-f6356ca9d75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----------+--------+-------+---+\n",
      "|channel|day|        seg|   setop|remains|row|\n",
      "+-------+---+-----------+--------+-------+---+\n",
      "|    NHK|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|    NHK|mon|       Game|ST_E_999|  10000|  2|\n",
      "|    NHK|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|    NHK|tue|       Game|ST_E_999|  10000|  4|\n",
      "|    NHK|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|    EBS|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|    EBS|mon|       Game|ST_E_999|  10000|  2|\n",
      "|    EBS|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|    EBS|tue|       Game|ST_E_999|  10000|  4|\n",
      "|    EBS|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|   TVCH|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|   TVCH|mon|       Game|ST_E_999|  10000|  2|\n",
      "|   TVCH|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|   TVCH|tue|       Game|ST_E_999|  10000|  4|\n",
      "|   TVCH|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|    CCN|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|    CCN|mon|       Game|ST_E_999|  10000|  2|\n",
      "|    CCN|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|    CCN|tue|       Game|ST_E_999|  10000|  4|\n",
      "|    CCN|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|GAME-TV|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|GAME-TV|mon|       Game|ST_E_999|  10000|  2|\n",
      "|GAME-TV|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|GAME-TV|tue|       Game|ST_E_999|  10000|  4|\n",
      "|GAME-TV|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|    KBS|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|    KBS|mon|       Game|ST_E_999|  10000|  2|\n",
      "|    KBS|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|    KBS|tue|       Game|ST_E_999|  10000|  4|\n",
      "|    KBS|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|    MBC|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|    MBC|mon|       Game|ST_E_999|  10000|  2|\n",
      "|    MBC|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|    MBC|tue|       Game|ST_E_999|  10000|  4|\n",
      "|    MBC|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|    CNN|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|    CNN|mon|       Game|ST_E_999|  10000|  2|\n",
      "|    CNN|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|    CNN|tue|       Game|ST_E_999|  10000|  4|\n",
      "|    CNN|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|Arirang|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|Arirang|mon|       Game|ST_E_999|  10000|  2|\n",
      "|Arirang|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|Arirang|tue|       Game|ST_E_999|  10000|  4|\n",
      "|Arirang|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "|    BTN|mon|Agriculture|ST_E_999|  10000|  1|\n",
      "|    BTN|mon|       Game|ST_E_999|  10000|  2|\n",
      "|    BTN|tue|Agriculture|ST_E_999|  10000|  3|\n",
      "|    BTN|tue|       Game|ST_E_999|  10000|  4|\n",
      "|    BTN|wed|Agriculture|ST_E_999|  10000|  5|\n",
      "+-------+---+-----------+--------+-------+---+\n",
      "only showing top 50 rows\n",
      "\n",
      "CPU times: user 10.2 ms, sys: 8.27 ms, total: 18.5 ms\n",
      "Wall time: 8.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ch 별로 setop 번호 순서 5개씩 보여주기\n",
    "# csv : 3.8 s, parq : 2.4s\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number \n",
    "\n",
    "windowPart = Window.partitionBy('channel').orderBy(col('setop').desc())\n",
    "lines = spark.read.format(FILE_FORMAT).schema(define_schema()).option('path', TABLE_NAME).load()\n",
    "df2 = lines.withColumn('row', row_number().over(windowPart)) .filter(col('row') <= 5)\n",
    "df2.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da9b1d-bcf5-4575-b54f-d83fb974e0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50db304-51a9-40e2-8e84-4e7c3516e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdb \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18870d96-93fd-437c-a3c8-b4c55c849348",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
