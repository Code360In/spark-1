{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1436e2-35dd-400b-86a3-fc5c2558fa36",
   "metadata": {},
   "source": [
    "# Hdfs remote connect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e3bb66-536c-4de4-9127-af54960d5454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.6/dist-packages (from pyspark)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyspark\n",
    "# pyspark가 설치된 경로를 확인해서 SPARK_HOME 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a582b451-4c10-4bc6-8dc2-30f25d11af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install findspark\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e555d5d7-0e20-46df-a5bc-a8c35d30bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "#import findspark\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38b5d68-823a-4172-8647-2905165191d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역변수 설정 \n",
    "# local mode \n",
    "SPARK_LOCAL_MASTER = \"local[3]\"\n",
    "# client mode \n",
    "SPARK_CLUSTER_MASTER = \"spark://34.125.237.158/:7077\" \n",
    "SPARK_APP_NAME = \"DATA-Preparation\"\n",
    "HOST_NAME = socket.gethostname()\n",
    "PORT = 9999\n",
    "\n",
    "# Define path\n",
    "DATA_ROOT = f'{os.getcwd()}/data'\n",
    "DATA_PATH = f'{DATA_ROOT}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658fcebc-e634-4c6b-964b-d996d4ea1306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-8-openjdk-amd64/\n",
      "/usr/local/lib/python3.6/dist-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "#os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64/'\n",
    "# os.environ['PATH'] = '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin/'\n",
    "#os.environ['SPARK_HOME'] = '/usr/local'\n",
    "#os.environ['SPARK_HOME']='/usr/local/lib/python3.6/dist-packages/pyspark'\n",
    "# os.environ['HADOOP_HOME'] = '/hadoop-3.2.2'\n",
    "# os.environ['hadoop.home.dir'] = '/hadoop-3.2.2/bin'\n",
    "# os.environ['CLASSPATH'] = '$CLASSPATH:/hadoop-3.2.2/spark-3.2.0-bin-hadoop3.2.tar'\n",
    "#findspark.init()\n",
    "print(os.getenv('JAVA_HOME'))\n",
    "print(os.getenv('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "267450f6-1b75-4726-8869-54dc61c8c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin/\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv('PATH'))\n",
    "#findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a42013b-ee43-4cc1-b0f2-b9fcdd037801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session 생성 메서드 \n",
    "def init_remote_session():\n",
    "    #SPARK_CLUSTER_MASTER = \"spark://34.64.108.172:7077\" \n",
    "    spark = SparkSession.builder.master(SPARK_CLUSTER_MASTER).appName(SPARK_APP_NAME).getOrCreate()\n",
    "    return spark\n",
    "    \n",
    "# local mode 실행 시 메모리를 확장해야 하는 경우 있어서, conf에 memory 변경 추가  \n",
    "def init_local_session():\n",
    "    #SPARK_LOCAL_MASTER = \"local[3]\"\n",
    "    spark = SparkSession.builder.master(SPARK_LOCAL_MASTER).appName(SPARK_APP_NAME).getOrCreate()\n",
    "    default_conf = spark.sparkContext._conf#.getAll()\n",
    "    print(f'Old Conf : {default_conf.getAll()}')\n",
    "    conf = spark.sparkContext._conf.setAll([\n",
    "        ('spark.executor.instances', 1)\n",
    "        #, ('spark.driver.memory', '12g'), ('spark.executor.memory', '8g'), ('spark.driver.maxResultSize', '8g')\n",
    "        , ('spark.driver.allowMultipleContexts', 'true'), ('spark.sql.shuffle.partitions', 8)\n",
    "        ##,('spark.memory.offHeap.enabled', True), ('spark.memory.offHeap.size', '8g')\n",
    "    ])\n",
    "    spark.sparkContext.stop()\n",
    "    \n",
    "    spark = SparkSession.builder.master(SPARK_LOCAL_MASTER).appName(SPARK_APP_NAME).config(conf=default_conf).getOrCreate()\n",
    "    new_conf = spark.sparkContext._conf\n",
    "    print(f'Updated Conf : {new_conf.getAll()}')\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d623878-4c1b-4466-9c20-0285756c191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_312\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n",
      "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98160a5-aa89-4558-bd5c-fb1cc89f503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Conf : [('spark.app.id', 'local-1644816980870'), ('spark.app.name', 'DATA-Preparation'), ('spark.sql.warehouse.dir', 'file:/notebooks/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.driver.host', 'spark-client'), ('spark.master', 'local[3]'), ('spark.app.startTime', '1644816979731'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.pyFiles', ''), ('spark.driver.port', '34379'), ('spark.executor.id', 'driver'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n",
      "Updated Conf : [('spark.sql.warehouse.dir', 'file:/notebooks/spark-warehouse'), ('spark.app.id', 'local-1644816982849'), ('spark.driver.host', 'spark-client'), ('spark.master', 'local[3]'), ('spark.app.startTime', '1644816982778'), ('spark.driver.allowMultipleContexts', 'true'), ('spark.executor.id', 'driver'), ('spark.app.name', 'DATA-Preparation'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.executor.instances', '1'), ('spark.submit.pyFiles', ''), ('spark.driver.port', '34379'), ('spark.submit.deployMode', 'client'), ('spark.sql.shuffle.partitions', '8'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-client:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DATA-Preparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[3] appName=DATA-Preparation>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스파크 생성 \n",
    "spark = init_local_session()\n",
    "# 스파크 클러스터를 실행해 두었다고 가정\n",
    "#spark = init_remote_session()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4682b3-7b4b-492a-8441-e3379febcf17",
   "metadata": {},
   "source": [
    "# HDFS Run\n",
    "Spark Driver와 다른 위치에서 Hadoop cluster를 실행하고, 이를 연결해서 사용해 본다.  \n",
    "172.17.0.3:9000 에 Hadoop 머신을 실행해 둔다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c65bd6-d0b9-4bf5-9de6-f6de9fe5b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.textFile(\"hdfs://172.17.0.3:9000/user/root/output\").textDF.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
