{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f66861-84ca-485a-bf52-7f78354319e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark==3.2.1\n",
    "# !pip install findspark\n",
    "# 컨테이너 레벨에 미리 설치해 두었다. \n",
    "# echo $PATH 로 /spark/bin 이 정상적으로 로드되서, pyspark 설치하지 않은 상태에서 명령창에서 pyspark로 실행 가능해야 한다.  \n",
    "# 여기서 설치하는 pyspark는 spark 연동 위한 api 용. \n",
    "# pyspark 명령창 실행은 주피터 실행 전에 환경 점검하고, 이를 그대로 넘겨 받는다.  \n",
    "# spark session은 결국 외부 명령창에서 pyspark를 실행하는 형태를 연결해 줄 뿐.  \n",
    "# pyspark 설치 전에 !which pyspark로 확인하면 : /spark/bin 이 출력되고, \n",
    "# 설치 후에 확인하면 /usr/local/bin 이 출력된다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6d6521-12f9-4d37-a231-3f03d8506d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin:/hadoop/bin:/hadoop/sbin:/spark/bin\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b51062e-08b7-4fc8-9265-2619d8222b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "import findspark\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7a35a4-775d-46a1-a9d5-c32dea1f05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역변수 설정 \n",
    "# local mode \n",
    "SPARK_LOCAL_MASTER = \"local[3]\"\n",
    "# client mode \n",
    "SPARK_CLUSTER_MASTER = \"yarn\" #\"spark://35.202.237.174:7077\" \n",
    "SPARK_APP_NAME = \"DATA-Preparation\"\n",
    "HOST_NAME = \"spark-master\" #socket.gethostname()\n",
    "\n",
    "\n",
    "# Define path\n",
    "DATA_ROOT = f'{os.getcwd()}/data'\n",
    "DATA_PATH = f'{DATA_ROOT}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479473a3-b1c1-435e-91b7-c818e2f61b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/data\n",
      "spark-master\n"
     ]
    }
   ],
   "source": [
    "print(DATA_PATH)\n",
    "print(HOST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8a280eb-a89c-4df1-abd7-af87d15ded16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['SPARK_HOME'] = \"/spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290d06e6-77a6-4210-aa44-3a3890abc6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spark\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "print(os.getenv('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465ff183-9ea9-468a-bbbf-a43218b303f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME : /usr/lib/jvm/java-8-openjdk-amd64\n",
      "PATH : /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin:/hadoop/bin:/hadoop/sbin:/spark/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/notebooks'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"JAVA_HOME : {os.getenv('JAVA_HOME')}\")\n",
    "print(f\"PATH : {os.getenv('PATH')}\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f725c2c3-9f7a-4c51-ba14-8d216c328702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session 생성 메서드 \n",
    "def init_remote_session():\n",
    "    #SPARK_CLUSTER_MASTER = \"spark://34.64.108.172:7077\" \n",
    "    spark = SparkSession.builder.master(SPARK_CLUSTER_MASTER).appName(SPARK_APP_NAME).getOrCreate()\n",
    "    return spark\n",
    "    \n",
    "# local mode 실행 시 메모리를 확장해야 하는 경우 있어서, conf에 memory 변경 추가  \n",
    "def init_local_session():\n",
    "    #SPARK_LOCAL_MASTER = \"local[3]\"\n",
    "    spark = SparkSession.builder.master(SPARK_LOCAL_MASTER).appName(SPARK_APP_NAME).config('spark.driver.host', HOST_NAME).getOrCreate()\n",
    "    default_conf = spark.sparkContext._conf#.getAll()\n",
    "    print(f'Old Conf : {default_conf.getAll()}')\n",
    "    conf = spark.sparkContext._conf.setAll([\n",
    "        ('spark.executor.instances', 1)\n",
    "        #, ('spark.driver.memory', '12g'), ('spark.executor.memory', '8g'), ('spark.driver.maxResultSize', '8g')\n",
    "        , ('spark.driver.allowMultipleContexts', 'true'), ('spark.sql.shuffle.partitions', 8)\n",
    "        ##,('spark.memory.offHeap.enabled', True), ('spark.memory.offHeap.size', '8g')\n",
    "    ])\n",
    "    spark.sparkContext.stop()\n",
    "    \n",
    "    spark = SparkSession.builder.master(SPARK_LOCAL_MASTER).appName(SPARK_APP_NAME).config(conf=default_conf).getOrCreate()\n",
    "    new_conf = spark.sparkContext._conf\n",
    "    print(f'Updated Conf : {new_conf.getAll()}')\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "679d0b86-4905-48ab-8587-5ea1bd0ff961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DATA-Preparation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=DATA-Preparation>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스파크 생성 \n",
    "#spark = init_local_session()\n",
    "# 스파크 클러스터를 실행해 두었다고 가정\n",
    "# spark = SparkSession.builder.master(\"yarn\").appName(SPARK_APP_NAME).getOrCreate()\n",
    "# spark standard cluster 처럼 url 직접 지정하지 않고 protocol만 지정해서, yarn-site.xml 에 설정된 yarn RM 에 연결한다.  \n",
    "spark = init_remote_session()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c94beeb-b877-4662-82f1-88a682cb256f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://namenode:9000/user/hive/warehouse/testdb.db/intbl",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b4bf40ca76b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# create table if not exists intbl ( eid int, ename string); insert into intbl values(1, 'a');\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 하이브 테이블이지만, 하이브로 접근하지 않았기 때문에 value : 1†a 형태로 읽힌다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://namenode:9000/user/hive/warehouse/testdb.db/intbl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     def csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://namenode:9000/user/hive/warehouse/testdb.db/intbl"
     ]
    }
   ],
   "source": [
    "# hive 에 testdb database 생성하고, intbl 테이블 생성 후, 1 row insert하고 조회.\n",
    "# create table if not exists intbl ( eid int, ename string); insert into intbl values(1, 'a');\n",
    "# 하이브 테이블이지만, 하이브로 접근하지 않았기 때문에 value : 1†a 형태로 읽힌다.  \n",
    "spark.read.text(\"hdfs://namenode:9000/user/hive/warehouse/testdb.db/intbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e118ba64-82f2-483e-879e-0af9d6e654d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b85db0-21db-43a5-85f4-20688d280ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
