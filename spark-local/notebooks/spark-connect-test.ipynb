{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f66861-84ca-485a-bf52-7f78354319e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/65/41eb22b7b4623d9f4560526cc456cb6425770c098a9dff6763111c4455cc/pyspark-3.2.1.tar.gz (281.4MB)\n",
      "\u001b[K    100% |################################| 281.4MB 4.3kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9.3 (from pyspark)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/e6/68db58a1d94d41ae042400f7965ed6a2c30e4108f77b54672d6451f86ebd/py4j-0.10.9.3-py2.py3-none-any.whl (198kB)\n",
      "\u001b[K    100% |################################| 204kB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/35/61/f720c76ff25c680383cd010cd524fa66c91c9b62a4b0aebe9f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453a4e13-fd14-48cc-a0bf-669ded03a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/cb/7d2bb508f4ca00a043fd53e8156c11767799d3f534bf451a0942211d5def/findspark-2.0.1-py2.py3-none-any.whl\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b51062e-08b7-4fc8-9265-2619d8222b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import socket\n",
    "import sys\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "import findspark\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf7a35a4-775d-46a1-a9d5-c32dea1f05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역변수 설정 \n",
    "# local mode \n",
    "SPARK_LOCAL_MASTER = \"local[3]\"\n",
    "# client mode \n",
    "SPARK_CLUSTER_MASTER = \"yarn\" #\"spark://35.202.237.174:7077\" \n",
    "SPARK_APP_NAME = \"DATA-Preparation\"\n",
    "HOST_NAME = \"spark-master\" #socket.gethostname()\n",
    "\n",
    "\n",
    "# Define path\n",
    "DATA_ROOT = f'{os.getcwd()}/data'\n",
    "DATA_PATH = f'{DATA_ROOT}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "479473a3-b1c1-435e-91b7-c818e2f61b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/data\n",
      "spark-master\n"
     ]
    }
   ],
   "source": [
    "print(DATA_PATH)\n",
    "print(HOST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290d06e6-77a6-4210-aa44-3a3890abc6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "findspark.init()\n",
    "print(os.getenv('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "465ff183-9ea9-468a-bbbf-a43218b303f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME : None\n",
      "PATH : /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/notebooks'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"JAVA_HOME : {os.getenv('JAVA_HOME')}\")\n",
    "print(f\"PATH : {os.getenv('PATH')}\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f725c2c3-9f7a-4c51-ba14-8d216c328702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session 생성 메서드 \n",
    "def init_remote_session():\n",
    "    #SPARK_CLUSTER_MASTER = \"spark://34.64.108.172:7077\" \n",
    "    spark = SparkSession.builder.master(SPARK_CLUSTER_MASTER).appName(SPARK_APP_NAME).getOrCreate()\n",
    "    return spark\n",
    "    \n",
    "# local mode 실행 시 메모리를 확장해야 하는 경우 있어서, conf에 memory 변경 추가  \n",
    "def init_local_session():\n",
    "    #SPARK_LOCAL_MASTER = \"local[3]\"\n",
    "    spark = SparkSession.builder.master(SPARK_LOCAL_MASTER).appName(SPARK_APP_NAME).config('spark.driver.host', HOST_NAME).getOrCreate()\n",
    "    default_conf = spark.sparkContext._conf#.getAll()\n",
    "    print(f'Old Conf : {default_conf.getAll()}')\n",
    "    conf = spark.sparkContext._conf.setAll([\n",
    "        ('spark.executor.instances', 1)\n",
    "        #, ('spark.driver.memory', '12g'), ('spark.executor.memory', '8g'), ('spark.driver.maxResultSize', '8g')\n",
    "        , ('spark.driver.allowMultipleContexts', 'true'), ('spark.sql.shuffle.partitions', 8)\n",
    "        ##,('spark.memory.offHeap.enabled', True), ('spark.memory.offHeap.size', '8g')\n",
    "    ])\n",
    "    spark.sparkContext.stop()\n",
    "    \n",
    "    spark = SparkSession.builder.master(SPARK_LOCAL_MASTER).appName(SPARK_APP_NAME).config(conf=default_conf).getOrCreate()\n",
    "    new_conf = spark.sparkContext._conf\n",
    "    print(f'Updated Conf : {new_conf.getAll()}')\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d0b86-4905-48ab-8587-5ea1bd0ff961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스파크 생성 \n",
    "#spark = init_local_session()\n",
    "# 스파크 클러스터를 실행해 두었다고 가정\n",
    "spark = init_remote_session()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87f692d0-563a-4580-9d52-3262fb6330dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
